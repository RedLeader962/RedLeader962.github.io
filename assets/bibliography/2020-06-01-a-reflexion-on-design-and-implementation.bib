@inproceedings{Henderson2018,
    abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
    archivePrefix = {arXiv},
    arxivId = {1709.06560},
    author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
    booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
    eprint = {1709.06560},
    file = {:Users/redleader/Desktop/Projet de lecture dirig {\' {e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Method, Technic {\&} know how/★ ★ ★ | Deep Reinforcement Learning that Matters.pdf:pdf},
    isbn = {9781577358008},
    pages = {3207--3214},
    url = {https://arxiv.org/abs/1709.06560},
    title = {Deep reinforcement learning that matters},
    year = {2018}
}
@article{Plappert2017,
    abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
    archivePrefix = {arXiv},
    arxivId = {1706.01905},
    author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
    eprint = {1706.01905},
    file = {:Users/redleader/Desktop/Projet de lecture dirig {\' {e}} 2019 - Luc Coupal/Lecture dirig {\' {e}} - lit {\' {e}} rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/⚠️ | Non-referenced in project proposal/Parameter space noise for exploration.pdf:pdf},
    pages = {1--18},
    title = {Parameter Space Noise for Exploration},
    url = {http://arxiv.org/abs/1706.01905},
    year = {2017}
}

@article{Duan2016,
    abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1604.06778v3},
    author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
    eprint = {arXiv:1604.06778v3},
    file = {:Users/redleader/Desktop/Projet de lecture dirig {\' {e}} 2019 - Luc Coupal/Lecture dirig {\' {e}} - lit {\' {e}} rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/⚠️ | Non-referenced in project proposal/Benchmarking Deep Reinforcement Learning for Continuous Control.pdf:pdf},
    isbn = {9781510829008},
    journal = {33rd International Conference on Machine Learning, ICML 2016},
    pages = {2001--2014},
    title = {Benchmarking deep reinforcement learning for continuous control},
    url = {https://arxiv.org/abs/1604.06778},
    volume = {3},
    year = {2016}
}

@article{Schulman2015a,
    abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
    archivePrefix = {arXiv},
    arxivId = {1502.05477},
    author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
    eprint = {1502.05477},
    file = {:Users/redleader/Desktop/Projet de lecture dirig {\' {e}} 2019 - Luc Coupal/Lecture dirig {\' {e}} - lit {\' {e}} rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/TRPO - Trust Region Policy Optimization.pdf:pdf},
    title = {Trust Region Policy Optimization},
    url = {http://arxiv.org/abs/1502.05477},
    year = {2015}
}

@article{Amiranashvili2018,
    abstract = {Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.},
    archivePrefix = {arXiv},
    arxivId = {1806.01175},
    author = {Amiranashvili, Artemij and Dosovitskiy, Alexey and Koltun, Vladlen and Brox, Thomas},
    eprint = {1806.01175},
    file = {:Users/redleader/Desktop/Projet de lecture dirig {\' {e}} 2019 - Luc Coupal/Lecture dirig {\' {e}} - lit {\' {e}} rature/depth knowledge/Fondation/RL vs DRL (Optional)/TD OR NOT TD - in DRL.pdf:pdf},
    number = {2016},
    pages = {1--14},
    title = {TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning},
    url = {http://arxiv.org/abs/1806.01175},
    year = {2018}
}
