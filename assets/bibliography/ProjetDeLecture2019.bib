Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Mohamed2019,
abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
archivePrefix = {arXiv},
arxivId = {1906.10652},
author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
eprint = {1906.10652},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/MDP and RL/üìñ ‚Üí Monte Carlo method/Monte Carlo Gradient Estimation in Machine Learning.pdf:pdf},
keywords = {gradient estimation,measure-valued estimator,monte carlo,pathwise estimator,score-function estimator,sensitivity analysis,variance reduction},
pages = {1--59},
title = {{Monte Carlo Gradient Estimation in Machine Learning}},
url = {http://arxiv.org/abs/1906.10652},
year = {2019}
}
@article{Peters2015,
author = {Peters, Jan and Neumann, Gerhard},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/üç´| breath knowledge (optional)/üèú ‚Üê PolicySearch - Methods and Applications.pdf:pdf},
pages = {1--108},
title = {{Policy Search: Methods and Applications (Overview Slide)}},
url = {http://icml.cc/2015/tutorials/PolicySearch.pdf},
year = {2015}
}
@article{Requirements,
author = {Requirements, Final Project},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/TP Inspiration/project{\_}assignment.pdf:pdf},
pages = {1--5},
title = {{CS294-112 Deep Reinforcement Learning : Final Project}}
}
@article{Eysenbach2018,
abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1802.06070},
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
eprint = {1802.06070},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/3-Unsupervised Meta-RL/DIAYN - Diversity is All You Need- Learning Skills without a Reward Function.pdf:pdf},
pages = {1--22},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
url = {http://arxiv.org/abs/1802.06070},
year = {2018}
}
@article{Mnih2016a,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.01783v2},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {arXiv:1602.01783v2},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/A3C - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@article{Wang2016a,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/1-Framework and key result/Deep Meta-RL - Learning to reinforcement learn.pdf:pdf},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@article{Schulman2015,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
eprint = {1506.02438},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/Architecture et am{\'{e}}lioration/GAE - high-dimensional continuous control using generalized advantage estimation.pdf:pdf},
pages = {1--14},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@article{Actor-critic2018a,
author = {Actor-critic, Soft},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/TP Inspiration/hw5a - advanced topic - Exploration.pdf:pdf},
number = {1},
pages = {1--9},
title = {{CS294-112 Deep Reinforcement Learning HW5a : Exploration}},
year = {2018}
}
@article{Hwangbo2017,
abstract = {In this paper, we present a method to control a quadrotor with a neural network trained using reinforcement learning techniques. With reinforcement learning, a common network can be trained to directly map state to actuator command making any predefined control structure obsolete for training. Moreover, we present a new learning algorithm which differs from the existing ones in certain aspects. Our algorithm is conservative but stable for complicated tasks. We found that it is more applicable to controlling a quadrotor than existing algorithms. We demonstrate the performance of the trained policy both in simulation and with a real quadrotor. Experiments show that our policy network can react to step response relatively accurately. With the same policy, we also demonstrate that we can stabilize the quadrotor in the air even under very harsh initialization (manually throwing it upside-down in the air with an initial velocity of 5 m/s). Computation time of evaluating the policy is only 7 {\{}$\backslash$mu{\}}s per time step which is two orders of magnitude less than common trajectory optimization algorithms with an approximated model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.05110v1},
author = {Hwangbo, J. and Sa, Inkyu and Siegwart, Roland and Hutter, Marco},
doi = {10.1109/LRA.2017.2720851},
eprint = {arXiv:1707.05110v1},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Robotic (optional)/Aerial/Control of a Quadrotor with Reinforcement Learning.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial systems: Mechanics and control,Learning and adaptive systems},
number = {4},
pages = {2096--2103},
title = {{Control of a Quadrotor with Reinforcement Learning}},
url = {https://www.youtube.com/watch?v=T0A9voXzhng},
volume = {2},
year = {2017}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1007/bf00992696},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/REINFORCE - Williams 1992 - SimpleStatisticalGradient-foll.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {connectionist networks,gradient descent,mathematical analysis,reinforcement learning},
number = {3-4},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@article{Anthony2017,
abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
archivePrefix = {arXiv},
arxivId = {1705.08439},
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
eprint = {1705.08439},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Dynamique connue/Expert iteration (Monte Carlo Tree search)/ExIt - Thinking Fast and slow with Deep Learning Tree search.pdf:pdf},
number = {Il},
pages = {1--19},
title = {{Thinking Fast and Slow with Deep Learning and Tree Search}},
url = {http://arxiv.org/abs/1705.08439},
year = {2017}
}
@article{Levine2014,
abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These tra-jectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with nu-merous contact discontinuities and underactuation.},
author = {Levine, Sergey and Abbeel, Pieter},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Apprendre une dynamique inconnue/Dynamique complexe ou difficile {\`{a}} apprendre/Optimisation de trajectoire/Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 27},
month = {jan},
pages = {1--3},
title = {{Learning Dynamic Manipulation Skills under Unknown Dynamics with Guided Policy Search}},
url = {http://papers.nips.cc/paper/5444-learning-neural-network-policies-with-guided-policy-search-under-unknown-dynamics.pdf},
year = {2014}
}
@article{Francois-Lavet2018,
abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
doi = {10.1561/2200000071},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
title = {{An Introduction to Deep Reinforcement Learning}},
year = {2018}
}
@article{Haarnoja2018a,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/Soft Actor-Critic/SAC - Soft Actor-Critic - Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/4-Exploration$\backslash$:exploitation/VIME - Variational Information Maximizing Exploration.pdf:pdf},
title = {{VIME: Variational Information Maximizing Exploration}},
url = {http://arxiv.org/abs/1605.09674},
year = {2016}
}
@article{Busoniu2010,
abstract = {Dynamic programming (DP) and reinforcement learning (RL) can be used to address problems from a variety of fields, including automatic control, artificial intelligence, operations research, and economy. Many problems in these fields are described by continuous variables, whereas DP and RL can find exact solutions only in the discrete case. Therefore, approximation is essential in practical DP and RL. This chapter provides an in-depth review of the literature on approximate DP and RL in large or continuous-space, infinite-horizon problems. Value iteration, policy iteration, and policy search approaches are presented in turn. Model-based (DP) as well as online and batch model-free (RL) algorithms are discussed. We review theoretical guarantees on the approximate solutions produced by these algorithms. Numerical examples illustrate the behavior of several representative algorithms in practice. Techniques to automatically derive value function approximators are discussed, and a comparison between value iteration, policy iteration, and policy search is provided. The chapter closes with a discussion of open issues and promising research directions in approximate DP and RL. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Bu≈üoniu, Lucian and {De Schutter}, Bart and Babu{\v{s}}ka, Robert},
doi = {10.1007/978-3-642-11688-9_1},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/MDP and RL/Dynamic programing/üìñ ‚Üí ‚òÖ ‚òÖ ‚òÖ |  Batch RL {\&} ADP (RLSS2018).pdf:pdf},
isbn = {9783642116872},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {3--44},
title = {{Approximate dynamic programming and reinforcement learning}},
volume = {281},
year = {2010}
}
@article{Gu2017,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.00633v2},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
doi = {10.1109/ICRA.2017.7989385},
eprint = {arXiv:1610.00633v2},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - 2017 - Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3389--3396},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}},
url = {https://www.youtube.com/watch?v=ZhsEKTo7V04},
year = {2017}
}
@article{Moore1990a,
abstract = {This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors-an approach which is formalized here as the AB (State-Action-Behaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall...},
author = {Moore, Andrew W},
doi = {10.1016/j.cmet.2017.06.019},
issn = {1932-7420},
journal = {Learning},
pmid = {28683280},
title = {{Efficient Memory-based Learning for Robot Control - Dissertation}},
year = {1990}
}
@article{Finn2016a,
abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
archivePrefix = {arXiv},
arxivId = {1611.03852},
author = {Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
eprint = {1611.03852},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Going deeper/A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models.pdf:pdf},
title = {{A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}},
url = {http://arxiv.org/abs/1611.03852},
year = {2016}
}
@article{Nagabandi2018,
abstract = {Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.},
archivePrefix = {arXiv},
arxivId = {1803.11347},
author = {Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S. and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
eprint = {1803.11347},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/2-Supervised Meta-RL/ReBAL, GrBAL - Learning to adapt in dynamic, real-world environments through meta-reinforcement learning.pdf:pdf},
pages = {1--17},
title = {{Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/1803.11347},
year = {2018}
}
@article{Duan2016,
abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1604.06778v3},
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
eprint = {arXiv:1604.06778v3},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/Benchmarking Deep Reinforcement Learning for Continuous Control.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2001--2014},
title = {{Benchmarking deep reinforcement learning for continuous control}},
volume = {3},
year = {2016}
}
@article{Sutton2017,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and deter- mining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, indepen- dent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor‚Äìcritic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
archivePrefix = {arXiv},
arxivId = {1706.06643},
author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
eprint = {1706.06643},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/Actor-critic (Optional)/classic paper/Sutton - Policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation Richard}},
url = {http://arxiv.org/abs/1706.06643},
volume = {12},
year = {2017}
}
@article{Kakade2002,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as deo/ned by Sutton et al. 9. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
author = {Kakade, Sham},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/(a classical paper) 2073-a-natural-policy-gradient.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{A natural policy gradient}},
year = {2002}
}
@article{Schulman2015a,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
eprint = {1502.05477},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/TRPO - Trust Region Policy Optimization.pdf:pdf},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol-icy gradient has a particularly appealing form: it is the expected gradient of the action-value func-tion. This simple form means that the deter-ministic policy gradient can be estimated much more efficiently than the usual stochastic pol-icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter-parts in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/DPG - Deterministic Policy Gradient Algorithms - silver.pdf:pdf},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
number = {1},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://proceedings.mlr.press/v32/silver14.pdf},
volume = {1},
year = {2014}
}
@article{Francois-Lavet2018a,
abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
archivePrefix = {arXiv},
arxivId = {1811.12560},
author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
month = {nov},
title = {{An Introduction to Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1811.12560 http://dx.doi.org/10.1561/2200000071},
year = {2018}
}
@article{DeepMind2019,
abstract = {Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a ‚Äúgrand challenge‚Äù for AI research.},
author = {DeepMind},
journal = {DeepMind},
keywords = {AlphaStar},
mendeley-tags = {AlphaStar},
title = {{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}},
url = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
year = {2019}
}
@article{Haarnoja2017,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
archivePrefix = {arXiv},
arxivId = {1702.08165},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
eprint = {1702.08165},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/Soft Q-Learning/Soft Q-Learning - Reinforcement Learning with Deep Energy-Based Policies.pdf:pdf},
title = {{Reinforcement Learning with Deep Energy-Based Policies}},
url = {http://arxiv.org/abs/1702.08165},
year = {2017}
}
@article{Actor-critic2018b,
author = {Actor-critic, Soft},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/TP Inspiration/hw5c  - advanced topic - Meta-Learning.pdf:pdf},
number = {1},
pages = {1--9},
title = {{CS294-112 Deep Reinforcement Learning HW5c : Meta-Reinforcement Learning}},
year = {2018}
}
@article{Degris2012,
abstract = {‚Äî Reinforcement learning methods are often con-sidered as a potential solution to enable a robot to adapt to changes in real time to an unpredictable environment. However, with continuous action, only a few existing algorithms are practical for real-time learning. In such a setting, most effective methods have used a parameterized policy structure, often with a separate parameterized value function. The goal of this paper is to assess such actor‚Äìcritic methods to form a fully specified practical algorithm. Our specific contributions include 1) developing the extension of existing incremental policy-gradient algorithms to use eligibility traces, 2) an empir-ical comparison of the resulting algorithms using continuous actions, 3) the evaluation of a gradient-scaling technique that can significantly improve performance. Finally, we apply our actor‚Äìcritic algorithm to learn on a robotic platform with a fast sensorimotor cycle (10ms). Overall, these results constitute an important step towards practical real-time learning control with continuous action.},
author = {Degris, Thomas and Pilarski, Patrick M. and Sutton, Richard S.},
doi = {10.1109/ACC.2012.6315022},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/Actor-critic (Optional)/Model-Free Reinforcement Learning with Continuous Action in Practice - Degris 2012.pdf:pdf},
isbn = {9781457710957},
issn = {07431619},
journal = {Proceedings of the American Control Conference},
number = {June 2014},
pages = {2177--2182},
title = {{Model-Free reinforcement learning with continuous action in practice}},
year = {2012}
}
@article{Ziebart2010,
abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy-an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks. Copyright 2010 by the author(s)/owner(s).},
author = {Ziebart, B.D. and Bagnell, J.A. and Dey, A.K.},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/going deeper/maximum-causal-entropy.pdf:pdf},
isbn = {9781605589077},
journal = {ICML 2010 - Proceedings, 27th International Conference on Machine Learning},
title = {{Modeling interaction via the principle of maximum causal entropy}},
year = {2010}
}
@article{Setup2018,
abstract = {The train{\_}PG.py file contains an incomplete implementation of policy gradient, and you will finish the implementation. The file has detailed instructions on which pieces you will write for this section. 4.1 Background 4.1.1 Reward to Go Recall that the policy gradient g can be expressed as the expectation of a few different expressions. These result in different ways of forming the sample estimate for g. Here, you will implement two ways, controlled by a flag in the code called reward-to-go. 1. Way one: trajectory-centric policy gradients, for which reward-to-go=False. Here, we compute g $\theta$ = E $\tau$ ‚àº$\pi$ $\theta$ [‚àá $\theta$ log P ($\tau$ |$\pi$ $\theta$)R($\tau$)] ‚âà 1 |D| $\tau$ ‚ààD ‚àá $\theta$ log P ($\tau$ |$\pi$ $\theta$)R($\tau$) = 1 |D| $\tau$ ‚ààD T t=0 ‚àá $\theta$ log $\pi$ $\theta$ (a t |s t) R($\tau$), where $\tau$ = (s 0 , a 0 , s 1 , ...) is a trajectory, D is a datset of trajectories collected on policy $\pi$ $\theta$ , $\theta$ is the set of parameters for the policy, and R($\tau$) = T t=0 $\gamma$ t r t is the discounted sum of rewards along a trajectory. 2. Way two: state/action-centric policy gradients, for which reward-to-go=True. 2 Here, we compute g $\theta$ = E $\tau$ ‚àº$\pi$ $\theta$ [ T t=0 $\gamma$ t ‚àá $\theta$ log $\pi$(a t |s t)Q $\pi$ (s t , a t) ‚âà 1 |D| $\tau$ ‚ààD T t=0 $\gamma$ t ‚àá $\theta$ log $\pi$(a t |s t) T t [ =t $\gamma$ t ‚àít r t . The flag reward-to-go refers to the fact that in this case, we push up the probability of picking action a t in state s t in proportion to the 'reward-to-go' from that state-action pair‚Äîthe sum of rewards achieved by starting in s t , taking action a t , and then acting according to the current policy forever after. 4.1.2 Advantage Normalization},
author = {Setup, Code},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Setup - 2018 - CS294-112 Deep Reinforcement Learning HW2 Policy Gradients.pdf:pdf},
pages = {1--6},
title = {{CS294-112 Deep Reinforcement Learning HW2 : Policy Gradients}},
year = {2018}
}
@misc{Silver2018,
abstract = {{\textless}p{\textgreater}The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.{\textless}/p{\textgreater}},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
booktitle = {Science},
doi = {10.1126/science.aar6404},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Game (optional)/alphazero{\_}preprint.pdf:pdf},
issn = {10959203},
number = {6419},
pages = {1140--1144},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
url = {https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf},
volume = {362},
year = {2018}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Vinyals2017,
abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
archivePrefix = {arXiv},
arxivId = {1708.04782},
author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
eprint = {1708.04782},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Game (optional)/StarCraft II - A New Challenge for Reinforcement Learning.pdf:pdf},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.04782},
year = {2017}
}
@article{Stadie2015,
abstract = {Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.},
archivePrefix = {arXiv},
arxivId = {1507.00814},
author = {Stadie, Bradly C. and Levine, Sergey and Abbeel, Pieter},
eprint = {1507.00814},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/4-Exploration$\backslash$:exploitation/Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models.pdf:pdf},
pages = {1--11},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}},
url = {http://arxiv.org/abs/1507.00814},
year = {2015}
}
@article{Hadfield-Menell2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/3-Reward shaping/Going deeper/CIRL - Cooperative Inverse Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Gupta2018,
abstract = {Meta-learning is a powerful tool that builds on multi-task learning to learn how to quickly adapt a model to new tasks. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by meta-learning prior tasks. The performance of meta-learning algorithms critically depends on the tasks available for meta-training: in the same way that supervised learning algorithms generalize best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We describe a general recipe for unsupervised meta-reinforcement learning, and describe an effective instantiation of this approach based on a recently proposed unsupervised exploration technique and model-agnostic meta-learning. We also discuss practical and conceptual considerations for developing unsupervised meta-learning methods. Our experimental results demonstrate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design, significantly exceeds the performance of learning from scratch, and even matches performance of meta-learning methods that use hand-specified task distributions.},
archivePrefix = {arXiv},
arxivId = {1806.04640},
author = {Gupta, Abhishek and Eysenbach, Benjamin and Finn, Chelsea and Levine, Sergey},
eprint = {1806.04640},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/3-Unsupervised Meta-RL/Unsupervised Meta-Learning for Reinforcement Learning.pdf:pdf},
title = {{Unsupervised Meta-Learning for Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.04640},
year = {2018}
}
@article{Yang2018,
author = {Yang, Guang-Zhong and Merrifield, Robert and Bellingham, Jim and Dupont, Pierre E. and Fischer, Peer and Floridi, Luciano and Taddeo, Mariarosaria and Full, Robert and Jacobstein, Neil and Kumar, Vijay and McNutt, Marcia and Nelson, Bradley J. and Scassellati, Brian and Taylor, Russell and Veloso, Manuela and Wang, Zhong Lin and Wood, Robert},
doi = {10.1126/scirobotics.aar7650},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/üç´| breath knowledge (optional)/Science robotics, Vol 3, Issue 14, 24 jan 2018 - The grand challenges of Science Robotics.pdf:pdf},
issn = {2470-9476},
journal = {Science Robotics},
keywords = {artificial intelligence,engineering,grand challenges,robotics,social robot,systems engineering,underpinning},
number = {14},
title = {{The grand challenges of science robotics}},
url = {https://robotics.sciencemag.org/content/3/14/eaar7650},
volume = {3},
year = {2018}
}
@article{Guo2014,
abstract = {The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best realtime agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.},
author = {Guo, X. and Singh, S. and Lee, H. and Lewis, R. and Wang, X.},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/üç´| breath knowledge (optional)/model-based (optional)/MTTS Case study - Imitation learning from MCTS (Optional)/MTCS - Deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {1--9},
title = {{Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning}},
volume = {4},
year = {2014}
}
@article{Wang2015,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
eprint = {1511.06581},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Value function (Q-Learning)/Architecture et am{\'{e}}lioration/Dueling  DQN - Dueling Network Architectures for Deep Reinforcement Learning.pdf:pdf},
number = {9},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06581},
year = {2015}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/PPO - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Finn2017a,
abstract = {Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.},
archivePrefix = {arXiv},
arxivId = {1710.11622},
author = {Finn, Chelsea and Levine, Sergey},
eprint = {1710.11622},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/1-Framework and key result/Meta-Learning and Universality- Deep Representations and Gradient Descent can Approximate any Learning Algorithm.pdf:pdf},
pages = {1--20},
title = {{Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm}},
url = {http://arxiv.org/abs/1710.11622},
year = {2017}
}
@article{Browne2012,
abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work. {\textcopyright} 2009 IEEE.},
author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
doi = {10.1109/TCIAIG.2012.2186810},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Dynamique connue/Expert iteration (Monte Carlo Tree search)/MCTS - A survey of Monte Carlo Tree Search Method.pdf:pdf},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Artificial intelligence (AI),Monte Carlo tree search (MCTS),bandit-based methods,computer Go,game search,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
number = {1},
pages = {1--43},
title = {{A survey of Monte Carlo tree search methods}},
url = {http://mcts.ai/pubs/mcts-survey-master.pdf},
volume = {4},
year = {2012}
}
@article{Fu2017,
abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
archivePrefix = {arXiv},
arxivId = {1710.11248},
author = {Fu, Justin and Luo, Katie and Levine, Sergey},
eprint = {1710.11248},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Approche antagoniste/AIRL - Learning robust rewards with adversarial inverse reinforcement learning.pdf:pdf},
pages = {1--15},
title = {{Learning Robust Rewards with Adversarial Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.11248},
year = {2017}
}
@article{Q-learning,
author = {Q-learning, The and Gym, Openai and Gym, Openai},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/TP Inspiration/hw3 - Q-Learning and Actor Critic.pdf:pdf},
pages = {1--8},
title = {{CS294-112 Deep Reinforcement Learning HW3 : Q-Learning and Actor-Critic Due October 10th , 11 : 59 pm}}
}
@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/Soft Actor-Critic/SAC improvment - Soft Actor-Critic Algorithms and Applications.pdf:pdf},
title = {{Soft Actor-Critic Algorithms and Applications}},
url = {http://arxiv.org/abs/1812.05905},
year = {2018}
}
@article{Gu2016,
abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
archivePrefix = {arXiv},
arxivId = {1611.02247},
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
eprint = {1611.02247},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/Architecture et am{\'{e}}lioration/Q-PROP - Sample-efficient policy gradient with an off-policy critic.pdf:pdf},
pages = {1--13},
title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
url = {http://arxiv.org/abs/1611.02247},
year = {2016}
}
@article{Parisotto2015,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
archivePrefix = {arXiv},
arxivId = {1511.06342},
author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.06342},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/5-Transfer learning/Actor-Mimic deep multitask and transfer reinforcement learning .pdf:pdf},
pages = {1--16},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06342},
year = {2015}
}
@article{Lewis2009,
abstract = {Reinforcement learning (RL) has achieved broad and successful application in cognitive science in part because of its gen- eral formulation of the adaptive control problem as the maximiza- tion of a scalar reward function. The computational RL framework is motivated by correspondences to animal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computational framework for reward that places it in an evolutionary context, formulating a notion of an optimal re- ward function given a fitness function and some distribution of envi- ronments. Novel results from computational experiments show how traditional notions of extrinsically and intrinsically motivated behav- iors may emerge from such optimal reward functions. In the experi- ments these rewards are discovered through automated search rather than crafted by hand. The precise form of the optimal reward func- tions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
author = {Lewis, Richard L and Singh, Satinder and Barto, Andrew G},
doi = {10.1.1.151.8250},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/3-Reward shaping/Where Do Rewards Come From?.pdf:pdf},
isbn = {1902956923},
journal = {Proceedings of the Annual Conference of the Cognitive Science Society},
pages = {2601--2606},
title = {{Where Do Rewards Come From?}},
url = {https://pdfs.semanticscholar.org/378c/76cb996770de0c605983490fd8f247cdfcb4.pdf},
year = {2009}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Dynamique connue/Expert iteration (Monte Carlo Tree search)/Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Abbeel2006,
abstract = {Autonomous helicopter flight is widely regarded to be a highly challenging control problem. This paper presents the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward flip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results significantly extend the state of the art in autonomous helicopter flight. We used the following approach: First we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to find a controller that is optimized for the resulting model and reward function. More specifically, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR).},
author = {Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew Y},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Robotic (optional)/Aerial/An Application of Reinforcement Learning to Aerobatic Helicopter Flight.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{An Application of Reinforcement Learning to Aerobatic Helicopter Flight}},
year = {2006}
}
@article{Amiranashvili2018,
abstract = {Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.},
archivePrefix = {arXiv},
arxivId = {1806.01175},
author = {Amiranashvili, Artemij and Dosovitskiy, Alexey and Koltun, Vladlen and Brox, Thomas},
eprint = {1806.01175},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/RL vs DRL (Optional)/TD OR NOT TD - in DRL.pdf:pdf},
number = {2016},
pages = {1--14},
title = {{TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01175},
year = {2018}
}
@article{Buckman2018,
abstract = {Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.},
archivePrefix = {arXiv},
arxivId = {1807.01675},
author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
eprint = {1807.01675},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Apprendre une dynamique inconnue/Model capable de tenir compte de l'incertitude/Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion.pdf:pdf},
number = {NeurIPS},
title = {{Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion}},
url = {http://arxiv.org/abs/1807.01675},
year = {2018}
}
@misc{Precup2000,
abstract = {Eligibility traces have been shown to speed re- $\backslash$n$\backslash$ninforcement learning, to make it more robust $\backslash$n$\backslash$nto hidden states, and to provide a link between $\backslash$n$\backslash$nMonte Carlo and temporal-difference methods. $\backslash$n$\backslash$nHere we generalize eligibility traces to off-policy $\backslash$n$\backslash$nlearning, in which one learns about a policy dif- $\backslash$n$\backslash$nferent from the policy that generates the data. $\backslash$n$\backslash$nOff-policy methods can greatly multiply learn- $\backslash$n$\backslash$ning, as many policies can be learned about from $\backslash$n$\backslash$nthe same data stream, and have been identified $\backslash$n$\backslash$nas particularly useful for learning about subgoals $\backslash$n$\backslash$nand temporally extended macro-actions. In this $\backslash$n$\backslash$npaper we consider the off-policy version of the $\backslash$n$\backslash$npolicy evaluation problem, for which only one $\backslash$n$\backslash$neligibility trace algorithm is known, a Monte $\backslash$n$\backslash$nCarlo method. We analyze and compare this and $\backslash$n$\backslash$nfour new eligibility trace algorithms, emphasiz- $\backslash$n$\backslash$ning their relationships to the classical statistical $\backslash$n$\backslash$ntechnique known as importance sampling. Our $\backslash$n$\backslash$nmain results are 1) to establish the consistency $\backslash$n$\backslash$nand bias properties of the new methods and 2) to $\backslash$n$\backslash$nempirically rank the new methods, showing im- $\backslash$n$\backslash$nprovement over one-step and Monte Carlo meth- $\backslash$n$\backslash$nods. Our results are restricted to model-free, $\backslash$n$\backslash$ntable-lookup methods and to offline updating (at $\backslash$n$\backslash$nthe end of each episode) although several of the $\backslash$n$\backslash$nalgorithms could be applied more generally.},
author = {Precup, Doina and Sutton, Richard S and Singh, Satinder P},
booktitle = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/Off-Policy Policy Gradient/Eligibility Traces for Off-Policy Policy Evaluation (Precup {\&} Sutton).pdf:pdf},
isbn = {1-55860-707-2},
keywords = {reinforcement},
pages = {759--766},
title = {{Eligibility Traces for Off-Policy Policy Evaluation}},
url = {http://scholarworks.umass.edu/cs{\_}faculty{\_}pubs{\%}5Cnhttp://scholarworks.umass.edu/cs{\_}faculty{\_}pubs/80},
year = {2000}
}
@article{Hasselt,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Value function (Q-Learning)/Architecture et am{\'{e}}lioration/Double DQN - Deep Reinforcement Learning with Double Q-Learning.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {Dependency parsing,Meta-features,Natural language processing,Part-of-speech tagging,Semi-supervised approach},
month = {sep},
pages = {173--191},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461},
volume = {230},
year = {2015}
}
@article{Wang2019,
abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.},
archivePrefix = {arXiv},
arxivId = {1901.01753},
author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
eprint = {1901.01753},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/4-Evolution strategies/Paired Open-Ended Trailblazer (POET)- Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions.pdf:pdf},
pages = {1--28},
title = {{Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions}},
url = {http://arxiv.org/abs/1901.01753},
year = {2019}
}
@article{Mishra2017,
abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
archivePrefix = {arXiv},
arxivId = {1707.03141},
author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
eprint = {1707.03141},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/2-Supervised Meta-RL/SNAIL - A simple neural attentive meta-learner.pdf:pdf},
pages = {1--17},
title = {{A Simple Neural Attentive Meta-Learner}},
url = {http://arxiv.org/abs/1707.03141},
year = {2017}
}
@article{Peters2008,
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g.,¬†by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Peters, Jan and Schaal, Stefan},
doi = {10.1016/j.neunet.2008.02.003},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/very accessible overview of optimal baselines and natural gradient/Reinforcement learning of motor skills with policy gradients.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Motor primitives,Motor skills,Natural Actor-Critic,Natural gradients,Policy gradient methods,Reinforcement learning},
number = {4},
pages = {682--697},
title = {{Reinforcement learning of motor skills with policy gradients}},
volume = {21},
year = {2008}
}
@inproceedings{Henderson2018,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1709.06560},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Method, Technic {\&} know how/‚òÖ ‚òÖ ‚òÖ | Deep Reinforcement Learning that Matters.pdf:pdf},
isbn = {9781577358008},
pages = {3207--3214},
title = {{Deep reinforcement learning that matters}},
year = {2018}
}
@article{Kormushev2010,
abstract = {We present an approach allowing a robot to acquire new motor skills by learning the couplings across motor control variables. The demonstrated skill is first encoded in a compact form through a modified version of Dynamic Movement Primitives (DMP) which encapsulates correlation information. Expectation-Maximization based Reinforcement Learning is then used to modulate the mixture of dynamical systems initialized from the user's demonstration. The approach is evaluated on a torque-controlled 7 DOFs Barrett WAM robotic arm. Two skill learning experiments are conducted: a reaching task where the robot needs to adapt the learned movement to avoid an obstacle, and a dynamic pancake-flipping task.},
author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G.},
doi = {10.1109/IROS.2010.5649089},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Robotic (optional)/Arm/Kormushev-IROS2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
pages = {3232--3237},
title = {{Robot motor skill coordination with EM-based reinforcement learning}},
year = {2010}
}
@article{Arel2010,
abstract = {A challenging application of artificial intelligence systems involves the scheduling of traffic signals in multi-intersection vehicular networks. This paper introduces a novel use of a multi-agent system and reinforcement learning (RL) framework to obtain an efficient traffic signal control policy. The latter is aimed at minimising the average delay, congestion and likelihood of intersection cross-blocking. A five-intersection traffic network has been studied in which each intersection is governed by an autonomous intelligent agent. Two types of agents, a central agent and an outbound agent, were employed. The outbound agents schedule traffic signals by following the longest-queue-first (LQF) algorithm, which has been proved to guarantee stability and fairness, and collaborate with the central agent by providing it local traffic statistics. The central agent learns a value function driven by its local and neighbours' traffic conditions. The novel methodology proposed here utilises the Q-Learning algorithm with a feedforward neural network for value function approximation. Experimental results clearly demonstrate the advantages of multi-agent RL-based control over LQF governed isolated single-intersection control, thus paving the way for efficient distributed traffic signal control in complex settings. {\textcopyright} 2010 The Institution of Engineering and Technology.},
author = {Arel, I. and Liu, C. and Urbanik, T. and Kohls, A. G.},
doi = {10.1049/iet-its.2009.0070},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/autre (optional)/IET{\_}ITS{\_}2010.pdf:pdf},
issn = {1751956X},
journal = {IET Intelligent Transport Systems},
number = {2},
pages = {128--135},
title = {{Reinforcement learning-based multi-agent system for network traffic signal control}},
volume = {4},
year = {2010}
}
@article{Wulfmeier2015,
abstract = {This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.},
archivePrefix = {arXiv},
arxivId = {1507.04888},
author = {Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
eprint = {1507.04888},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Maximum Entropy DEEP Inverse Reinforcement Learning.pdf:pdf},
title = {{Maximum Entropy Deep Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1507.04888},
year = {2015}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Apprenticeship Learning via Inverse Reinforcement Learning.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning - ICML '04},
number = {346},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@article{Actor-critic2018,
author = {Actor-critic, Soft},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/TP Inspiration/hw5b - advanced topic - soft Actor-Critic.pdf:pdf},
number = {1},
pages = {1--11},
title = {{CS294-112 Deep Reinforcement Learning HW5b : Soft Actor-Critic}},
year = {2018}
}
@misc{OpenAI2019,
abstract = {OpenAI Five is the first AI to beat the world champions in an esports game after defeating the reigning Dota 2 world champions, OG, at the OpenAI Five Finals on April 13, 2019.},
author = {OpenAI},
title = {{OpenAI Five}},
url = {https://openai.com/five/},
year = {2019}
}
@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Dynamique connue/Expert iteration (Monte Carlo Tree search)/AlphaZero - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@article{Levine2015a,
abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
author = {Levine, Sergey and Koltun, Vladlen},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/Architecture et am{\'{e}}lioration/Guided Policy Search - Importance sampling.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1--9},
title = {{Guided Policy Search}},
url = {http://proceedings.mlr.press/v28/levine13.html},
volume = {28},
year = {2013}
}
@article{Wang2016,
abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
eprint = {1611.01224},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/Architecture et am{\'{e}}lioration/ACER - Sample efficient actor-critic with experience replay.pdf:pdf},
number = {2016},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2016}
}
@article{Rusu2015,
abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
archivePrefix = {arXiv},
arxivId = {1511.06295},
author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
eprint = {1511.06295},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/5-Transfer learning/Policy distillation.pdf:pdf},
pages = {1--13},
title = {{Policy Distillation}},
url = {http://arxiv.org/abs/1511.06295},
year = {2015}
}
@article{Bellemare2016,
abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
archivePrefix = {arXiv},
arxivId = {1606.01868},
author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
eprint = {1606.01868},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/4-Exploration$\backslash$:exploitation/Unifying Count-Based Exploration and Intrinsic Motivation.pdf:pdf},
number = {Nips},
title = {{Unifying Count-Based Exploration and Intrinsic Motivation}},
url = {http://arxiv.org/abs/1606.01868},
year = {2016}
}
@article{Salimans2017,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
eprint = {1703.03864},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/‚ö†Ô∏è | Non-referenced in project proposal/‚òÖ ‚òÖ ‚òÖ | Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:pdf},
pages = {1--13},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.03864},
year = {2017}
}
@article{Tang2010,
abstract = {Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U() at the current policy parameterization , rather than to obtain a more complete estimate of U(), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines‚Äîa new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.},
author = {Tang, Jie and Abbeel, Pieter},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/Off-Policy Policy Gradient/‚òÖ ‚òÖ ‚òÖ | On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010},
pages = {1--9},
title = {{On a connection between importance sampling and the likelihood ratio policy gradient}},
year = {2010}
}
@book{Sutton1394,
address = {Cambridge, MA},
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2},
editor = {{MIT Press}},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Sutton, Barto - 2018 - Reinforcement learning An introduction.pdf:pdf},
isbn = {978-0262039246},
title = {{Reinforcement learning: An introduction}},
url = {http://incompleteideas.net/book/RLbook2018.pdf},
year = {2018}
}
@article{Levine2018,
abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {1805.00909},
author = {Levine, Sergey},
eprint = {1805.00909},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/Reinforcement Learning and Control as Probabilistic Inference - Tutorial and Review - Levine.pdf:pdf},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
url = {http://arxiv.org/abs/1805.00909},
year = {2018}
}
@article{Heess2017,
abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx{\_}bgoTF7bs .},
archivePrefix = {arXiv},
arxivId = {1707.02286},
author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
eprint = {1707.02286},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Robotic (optional)/Locomotion/Emergence of Locomotion Behaviours in Rich Environments.pdf:pdf},
title = {{Emergence of Locomotion Behaviours in Rich Environments}},
url = {http://arxiv.org/abs/1707.02286},
year = {2017}
}
@article{Wang2019a,
abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/{\~{}}tingwuwang/mbrl.html.},
archivePrefix = {arXiv},
arxivId = {1907.02057},
author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
eprint = {1907.02057},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /‚ö†Ô∏è | intro et intuition (Non-referenced in project proposal)/‚òÖ ‚òÖ ‚òÖ | Benchmarking Model-Based Reinforcement Learning/Benchmarking Model-Based Reinforcement Learning.pdf:pdf},
pages = {1--25},
title = {{Benchmarking Model-Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1907.02057},
year = {2019}
}
@article{Mnih2015a,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf},
volume = {518},
year = {2015}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Value function (Q-Learning)/DQN - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Houthooft2018,
abstract = {We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.04821},
author = {Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
eprint = {1802.04821},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/4-Evolution strategies/Evolved Policy Gradients.pdf:pdf},
number = {NeurIPS},
pages = {1--10},
title = {{Evolved Policy Gradients}},
url = {http://arxiv.org/abs/1802.04821},
year = {2018}
}
@article{Thomas2014,
author = {Thomas, Philip S},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Thomas - 2014 - Thomas14.pdf:pdf},
pages = {1--8},
title = {{Bias in Natural Actor-Critic algorithme}},
url = {papers://d471b97a-e92c-44c2-8562-4efc271c8c1b/Paper/p261},
volume = {32},
year = {2014}
}
@article{Hadfield-Menell2017,
abstract = {Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.},
archivePrefix = {arXiv},
arxivId = {1711.02827},
author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
eprint = {1711.02827},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/3-Reward shaping/Inverse Reward Design.pdf:pdf},
number = {Nips},
title = {{Inverse Reward Design}},
url = {http://arxiv.org/abs/1711.02827},
year = {2017}
}
@article{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
eprint = {1603.00448},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Guided Cost Learning- Deep Inverse Optimal Control via Policy Optimization.pdf:pdf},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {http://arxiv.org/abs/1603.00448},
volume = {48},
year = {2016}
}
@article{Plappert2017,
abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
archivePrefix = {arXiv},
arxivId = {1706.01905},
author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
eprint = {1706.01905},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/Parameter space noise for exploration.pdf:pdf},
pages = {1--18},
title = {{Parameter Space Noise for Exploration}},
url = {http://arxiv.org/abs/1706.01905},
year = {2017}
}
@inproceedings{Ziebart2008,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright ? 2008.},
author = {Ziebart, B.D. and Maas, Andrew and Bagnell, J.A. and Dey, A.K.},
booktitle = {Proc. AAAI},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/2-Inverse RL/Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
title = {{Maximum entropy inverse reinforcement learning}},
url = {https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
year = {2008}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.03400},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/2-Supervised Meta-RL/MAML - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:pdf},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {http://arxiv.org/abs/1703.03400},
year = {2017}
}
@article{Devin2017,
abstract = {Reinforcement learning (RL) can automate a wide variety of robotic skills, but learning each new skill requires considerable real-world data collection and manual representation engineering to design policy classes or features. Using deep reinforcement learning to train general purpose neural network policies alleviates some of the burden of manual representation engineering by using expressive policy classes, but exacerbates the challenge of data collection, since such methods tend to be less efficient than RL with low-dimensional, hand-designed representations. Transfer learning can mitigate this problem by enabling us to transfer information from one skill to another and even from one robot to another. We show that neural network policies can be decomposed into "task-specific" and "robot-specific" modules, where the task-specific modules are shared across robots, and the robot-specific modules are shared across all tasks on that robot. This allows for sharing task information, such as perception, between robots and sharing robot information, such as dynamics and kinematics, between tasks. We exploit this decomposition to train mix-and-match modules that can solve new robot-task combinations that were not seen during training. Using a novel neural network architecture, we demonstrate the effectiveness of our transfer method for enabling zero-shot generalization with a variety of robots and tasks in simulation for both visual and non-visual tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.07088v1},
author = {Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
doi = {10.1109/ICRA.2017.7989250},
eprint = {arXiv:1609.07088v1},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/5-Transfer learning/Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2169--2176},
title = {{Learning modular neural network policies for multi-task and multi-robot transfer}},
year = {2017}
}
@article{Moravcik2017,
abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.01724v1},
author = {Moravcik, Matej and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
doi = {10.1126/science.aam6960.1},
eprint = {arXiv:1701.01724v1},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Game (optional)/DeepStack.pdf:pdf},
journal = {Science},
number = {May},
pages = {1--32},
title = {{DeepStack : Expert-Level Artificial Intelligence in No-Limit Poker}},
url = {http://science.sciencemag.org/},
volume = {513},
year = {2017}
}
@article{Barto1983,
abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
doi = {10.1109/TSMC.1983.6313077},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {5},
pages = {834--846},
title = {{Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems}},
volume = {SMC-13},
year = {1983}
}
@article{Mao2016,
abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-oftheart heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
doi = {10.1145/3005745.3005750},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/autre (optional)/deeprm-hotnets16.pdf:pdf},
isbn = {9781450346610},
journal = {HotNets 2016 - Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {50--56},
title = {{Resource management with deep reinforcement learning}},
year = {2016}
}
@article{Schulman2016,
author = {Schulman, John},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Nuts and Bolts of Deep RL Experimentation/‚òÖ ‚òÖ ‚òÖ | nuts-and-bolts.pdf:pdf},
title = {{NIPS 2016 Tutorial: The Nuts and Bolts of Deep RL Research}},
url = {http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf},
year = {2016}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Actor-Critic/DDPG - Continuous control with deep reinforcement learning.pdf:pdf},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Schulman2017a,
abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and {\$}Q{\$}-learning methods. {\$}Q{\$}-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the {\$}Q{\$}-values they estimate are very inaccurate. A partial explanation may be that {\$}Q{\$}-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between {\$}Q{\$}-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) {\$}Q{\$}-learning is exactly equivalent to a policy gradient method. We also point out a connection between {\$}Q{\$}-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of {\$}Q{\$}-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a {\$}Q{\$}-learning method that closely matches the learning dynamics of A3C without using a target network or {\$}\backslashepsilon{\$}-greedy exploration schedule.},
archivePrefix = {arXiv},
arxivId = {1704.06440},
author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
eprint = {1704.06440},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/1-Maximum Entropy RL/Soft Q-Learning/Equivalence Between Policy Gradients and Soft Q-Learning.pdf:pdf},
pages = {1--15},
title = {{Equivalence Between Policy Gradients and Soft Q-Learning}},
url = {http://arxiv.org/abs/1704.06440},
year = {2017}
}
@article{XUEBIN2017,
abstract = {Loop-closure detection on 3D data is a challenging task that has been commonly approached by adapting image-based solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable loop-closure detection algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of "perfect segmentation", or on the existence of "objects" in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm will be made available after publication.},
author = {{XUE BIN}, PENG and GLEN, BERSETH and KANGKANG, YIN and MICHIEL, VAN DE PANNE},
doi = {10.1145/3072959.3073602},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Robotic (optional)/Locomotion/DeepLoco- Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning .pdf:pdf},
isbn = {9781450342797},
journal = {ACM Transactions on Graphics},
number = {4},
title = {{DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning}},
url = {http://www.cs.ubc.ca/{~}van/papers/2017-TOG-deepLoco/2017-TOG-deepLoco.pdf https://www.cs.ubc.ca/{~}van/papers/2017-TOG-deepLoco/},
volume = {36},
year = {2017}
}
@article{Ortega2019,
abstract = {In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.},
archivePrefix = {arXiv},
arxivId = {1905.03030},
author = {Ortega, Pedro A. and Wang, Jane X. and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M. and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and Gy{\"{o}}rgy, Andr{\'{a}}s and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and van Hasselt, Hado and de Freitas, Nando and Botvinick, Matthew and Legg, Shane},
eprint = {1905.03030},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Notions Avanc{\'{e}}s/6-Meta-RL/Meta-learning of Sequential Strategies - DeepMind.pdf:pdf},
pages = {1--15},
title = {{Meta-learning of Sequential Strategies}},
url = {http://arxiv.org/abs/1905.03030},
year = {2019}
}
@article{Levine2015,
abstract = {Autonomous learning of object manipulation skills can enable robots to acquire rich behavioral repertoires that scale to the variety of objects found in the real world. However, current motion skill learning methods typically restrict the behavior to a compact, low-dimensional representation, limiting its expressiveness and generality. In this paper, we extend a recently developed policy search method $\backslash$cite{\{}la-lnnpg-14{\}} and use it to learn a range of dynamic manipulation behaviors with highly general policy representations, without using known models or example demonstrations. Our approach learns a set of trajectories for the desired motion skill by using iteratively refitted time-varying linear models, and then unifies these trajectories into a single control policy that can generalize to new situations. To enable this method to run on a real robot, we introduce several improvements that reduce the sample count and automate parameter selection. We show that our method can acquire fast, fluent behaviors after only minutes of interaction time, and can learn robust controllers for complex tasks, including putting together a toy airplane, stacking tight-fitting lego blocks, placing wooden rings onto tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps onto bottles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.05611v2},
author = {Levine, Sergey and Wagener, Nolan and Abbeel, Pieter},
doi = {10.1109/ICRA.2015.7138994},
eprint = {arXiv:1501.05611v2},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Apprendre une dynamique inconnue/Model local et iterative LQR/ Learning Contact-Rich Manipulation Skills with Guided Policy Search.pdf:pdf},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {156--163},
title = {{Learning contact-rich manipulation skills with guided policy search}},
volume = {2015-June},
year = {2015}
}
@article{Degris2012a,
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4839v5},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
eprint = {arXiv:1205.4839v5},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-Free (Dynamique inconnue)/Policy optimization/‚ö†Ô∏è | Non-referenced in project proposal/Off-Policy Policy Gradient/Off-Policy Actor-Critic (ref OFF-policy Policy Gradient theorem).pdf:pdf},
isbn = {9781450312851},
journal = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
pages = {457--464},
title = {{Off-policy actor-critic}},
volume = {1},
year = {2012}
}
