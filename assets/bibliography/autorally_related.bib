
@article{Levine2020,
abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
archivePrefix = {arXiv},
arxivId = {2005.01643},
author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
eprint = {2005.01643},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2020 - Offline reinforcement learning Tutorial, review, and perspectives on open problems(2).pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--43},
title = {{Offline reinforcement learning: Tutorial, review, and perspectives on open problems}},
year = {2020}
}
@article{Haarnoja2018d,
abstract = {Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.},
archivePrefix = {arXiv},
arxivId = {1803.06773},
author = {Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
doi = {10.1109/ICRA.2018.8460756},
eprint = {1803.06773},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Composable Deep Reinforcement Learning for Robotic Manipulation.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {1},
pages = {6244--6251},
title = {{Composable Deep Reinforcement Learning for Robotic Manipulation}},
year = {2018}
}

@article{Ostafew2016,
abstract = {This paper presents a Robust Constrained Learning-based Nonlinear Model Predictive Control (RC-LB-NMPC) algo- rithm for path-tracking in off-road terrain. For mobile robots, constraints may represent solid obstacles or localization limits. As a result, constraint satisfaction is required for safety. Constraint satisfaction is typically guaranteed through the use of accurate, a priori models or robust control. However, accurate models are generally not available for off-road operation. Furthermore, robust controllers are often conservative, since model uncertainty is not updated online. In this work our goal is to use learning to generate low-uncertainty, non-parametric models in situ. Based on these models, the predictive controller computes both linear and angular velocities in real-time, such that the robot drives at or near its capabilities while respecting path and localization constraints. Localization for the controller is provided by an on-board, vision-based mapping and navigation system enabling operation in large-scale, off-road environments. The paper presents experimental results, including over 5 km of travel by a 900 kg skid-steered robot at speeds of up to 2.0 m/s. The result is a robust, learning controller that provides safe, conservative control during initial trials when model uncertainty is high and converges to high-performance, optimal control during later trials when model uncertainty is reduced with experience.},
author = {Ostafew, Chris J. and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1177/0278364916645661},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Ostafew, Schoellig, Barfoot - 2016 - Robust Constrained Learning-based NMPC enabling reliable mobile robot path tracking.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Gaussian process,Model predictive control,robust control},
month = {nov},
number = {13},
pages = {1547--1563},
publisher = {SAGE Publications Inc.},
title = {{Robust Constrained Learning-based NMPC enabling reliable mobile robot path tracking}},
volume = {35},
year = {2016}
}
@article{Goldfain2019,
author = {Goldfain, Brian and Drews, Paul and You, Changxi and Barulic, Matthew and Velev, Orlin and Tsiotras, Panagiotis and Rehg, James M.},
doi = {10.1109/MCS.2018.2876958},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Goldfain et al. - 2019 - AutoRally An Open Platform for Aggressive Autonomous Driving.pdf:pdf},
issn = {1066-033X},
journal = {IEEE Control Systems},
month = {feb},
number = {1},
pages = {26--55},
publisher = {IEEE},
title = {{AutoRally: An Open Platform for Aggressive Autonomous Driving}},
url = {https://ieeexplore.ieee.org/document/8616931/},
volume = {39},
year = {2019}
}

@article{Seegmiller2016,
abstract = {Advances in hardware design have made wheeled mobile robots (WMRs) exceptionally mobile. To fully exploit this mobility, we present a novel dynamic model formulation for use in WMR planning, control, and estimation systems. The formulation is high fidelity, general, modular, and fast. It builds on our prior work on recursive methods for 3-D kinematics derivation and constrained motion prediction using differential algebraic equations. It is stable, even for large integration steps, and can enforce realistic nonlinear models of wheel-terrain interaction. Simulation tests show our dynamic models to be more functional, stable, and efficient than common alternatives. Simulations can run over 1K× faster than real time on an ordinary PC. Experimental results on multiple platforms and terrain types show that, once calibrated, our models predict motion accurately. To facilitate their use, we have released open-source MATLAB and C++ libraries implementing our modeling/simulation methods.},
author = {Seegmiller, Neal and Kelly, Alonzo},
doi = {10.1109/TRO.2016.2546310},
file = {::},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Dynamics,kinematics,model identification,wheel slip,wheel-terrain interaction,wheeled mobile robots},
number = {3},
pages = {614--625},
publisher = {IEEE},
title = {{High-Fidelity Yet Fast Dynamic Models of Wheeled Mobile Robots}},
volume = {32},
year = {2016}
}
@inproceedings{Williams2016,
abstract = {In this paper we present a model predictive control algorithm designed for optimizing non-linear systems subject to complex cost criteria. The algorithm is based on a stochastic optimal control framework using a fundamental relationship between the information theoretic notions of free energy and relative entropy. The optimal controls in this setting take the form of a path integral, which we approximate using an efficient importance sampling scheme. We experimentally verify the algorithm by implementing it on a Graphics Processing Unit (GPU) and apply it to the problem of controlling a fifth-scale Auto-Rally vehicle in an aggressive driving task.},
author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487277},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Williams et al. - 2016 - Aggressive driving with model predictive path integral control.pdf:pdf},
isbn = {978-1-4673-8026-3},
issn = {10504729},
month = {may},
pages = {1433--1440},
publisher = {IEEE},
title = {{Aggressive driving with model predictive path integral control}},
url = {https://ieeexplore.ieee.org/document/7487277/},
volume = {2016-June},
year = {2016}
}
@article{Williams2018,
abstract = {We present an information theoretic approach to stochastic optimal control problems that can be used to derive general sampling based optimization schemes. This new mathematical method is used to develop a sampling based model predictive control algorithm. We apply this information theoretic model predictive control (IT-MPC) scheme to the task of aggressive autonomous driving around a dirt test track, and compare its performance to a model predictive control version of the cross-entropy method.},
author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
doi = {10.1109/TRO.2018.2865891},
file = {::},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Adaptive Systems,Autonomous Vehicles,Control Architectures and Programming,Learning,Motion Control,Stochastic Optimal Control},
month = {dec},
number = {6},
pages = {1603--1622},
title = {{Information-Theoretic Model Predictive Control: Theory and Applications to Autonomous Driving}},
url = {https://ieeexplore.ieee.org/document/8558663/},
volume = {34},
year = {2018}
}
@article{Eysenbach2021,
abstract = {Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.},
archivePrefix = {arXiv},
arxivId = {2103.06257},
author = {Eysenbach, Benjamin and Levine, Sergey},
eprint = {2103.06257},
file = {:Users/redleader/Desktop/MaxEnt-RL and Robust RL/Maximum Entropy RL (Provably) Solves Some Robust RL Problems v1.pdf:pdf},
title = {{Maximum Entropy RL (Provably) Solves Some Robust RL Problems}},
url = {http://arxiv.org/abs/2103.06257},
year = {2021}
}
@inproceedings{Williams2017,
abstract = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M. and Boots, Byron and Theodorou, Evangelos A.},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989202},
file = {::},
isbn = {978-1-5090-4633-1},
issn = {10504729},
month = {may},
pages = {1714--1721},
publisher = {IEEE},
title = {{Information theoretic MPC for model-based reinforcement learning}},
url = {https://ieeexplore.ieee.org/document/7989202/},
year = {2017}
}
@article{Ordonez2017,
abstract = {Modeling of the motion of a skid-steered robot is challenging since slippage and skidding is inherent to this type of platform and it requires high torques to perform curvilinear motion. If the ground–robot interaction and torque requirements are not captured properly, motion planners will sometimes generate trajectories that are not achievable by the robot. Important motion planning applications that rely heavily in these models, include energy efficient and momentum based planning. However, these models change as the terrain surface varies. To cope with this issue, this paper presents a methodology to perform online learning of such models. It combines detailed slip and terramechanic-based dynamic models of wheel–terrain interaction with online learning via Extended Kalman filtering (to update the kinematic model) and an efficient neural network formulation (to update the dynamic model). The proposed approach experimentally demonstrates the importance of the joint utilization of the learned vehicle models in the context of energy efficient motion planning. In particular, the slip-enhanced kinematic models are used to efficiently provide estimates of robot pose and the dynamic models are employed to generate energy estimates and minimum turn radius constraints.},
author = {Ordonez, Camilo and Gupta, Nikhil and Reese, Brandon and Seegmiller, Neal and Kelly, Alonzo and Collins, Emmanuel G.},
doi = {10.1016/j.robot.2017.05.014},
file = {::},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Energy efficient planning,Online learning,Skid-steered robots,Wheel–terrain interaction},
month = {sep},
pages = {207--221},
publisher = {Elsevier B.V.},
title = {{Learning of skid-steered kinematic and dynamic models for motion planning}},
url = {http://dx.doi.org/10.1016/j.robot.2017.05.014 https://linkinghub.elsevier.com/retrieve/pii/S0921889015302207},
volume = {95},
year = {2017}
}
@inproceedings{Nagariya2020,
abstract = {In this work we evaluate Iterative Linear Quadratic Regulator(ILQR) for trajectory tracking of two different kinds of wheeled mobile robots namely Warthog (Fig. 1), an off-road holonomic robot with skid-steering and Polaris GEM e6 [1], a non-holonomic six seater vehicle (Fig. 2). We use multilayer neural network to learn the discrete dynamic model of these robots which is used in ILQR controller to compute the control law. We use model predictive control (MPC) to deal with model imperfections and perform extensive experiments to evaluate the performance of the controller on human driven reference trajectories with vehicle speeds of 3m/s- 4m/s for warthog and 7m/s-10m/s for the Polaris GEM},
archivePrefix = {arXiv},
arxivId = {2007.14492},
author = {Nagariya, Akhil and Saripalli, Srikanth},
booktitle = {2020 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IV47402.2020.9304851},
eprint = {2007.14492},
file = {::},
isbn = {978-1-7281-6673-5},
month = {oct},
pages = {1740--1745},
publisher = {IEEE},
title = {{An Iterative LQR Controller for Off-Road and On-Road Vehicles using a Neural Network Dynamics Model}},
url = {http://arxiv.org/abs/2007.14492 https://ieeexplore.ieee.org/document/9304851/},
year = {2020}
}
@article{Srinivasan2020,
abstract = {Velocity estimation plays a central role in driverless vehicles, but standard and affordable methods struggle to cope with extreme scenarios like aggressive maneuvers due to the presence of high sideslip. To solve this, autonomous race cars are usually equipped with expensive external velocity sensors. In this paper, we present an end-to-end recurrent neural network that takes available raw sensors as input (IMU, wheel odometry, and motor currents) and outputs velocity estimates. The results are compared to two state-of-the-art Kalman filters, which respectively include and exclude expensive velocity sensors. All methods have been extensively tested on a formula student driverless race car with very high sideslip (10{\{}$\backslash$deg{\}} at the rear axle) and slip ratio ({\~{}}20{\%}), operating close to the limits of handling. The proposed network is able to estimate lateral velocity up to 15x better than the Kalman filter with the equivalent sensor input and matches (0.06 m/s RMSE) the Kalman filter with the expensive velocity sensor setup.},
archivePrefix = {arXiv},
arxivId = {2003.06917},
author = {Srinivasan, Sirish and Sa, Inkyu and Zyner, Alex and Reijgwart, Victor and Valls, Miguel I. and Siegwart, Roland},
doi = {10.1109/lra.2020.3016929},
eprint = {2003.06917},
file = {::},
journal = {IEEE Robotics and Automation Letters},
number = {4},
pages = {6869--6875},
title = {{End-to-End Velocity Estimation for Autonomous Racing}},
volume = {5},
year = {2020}
}
@article{Wagener2019,
abstract = {Model predictive control (MPC) is a powerful technique for solving dynamic control tasks. In this paper, we show that there exists a close connection between MPC and online learning, an abstract theoretical framework for analyzing online decision making in the optimization literature. This new perspective provides a foundation for leveraging powerful online learning algorithms to design MPC algorithms. Specifically, we propose a new algorithm based on dynamic mirror descent (DMD), an online learning algorithm that is designed for non-stationary setups. Our algorithm, Dynamic Mirror Descent Model Predictive Control (DMD-MPC), represents a general family of MPC algorithms that includes many existing techniques as special instances. DMDMPC also provides a fresh perspective on previous heuristics used in MPC and suggests a principled way to design new MPC algorithms. In the experimental section of this paper, we demonstrate the flexibility of DMD-MPC, presenting a set of new MPC algorithms on a simple simulated cartpole and a simulated and real-world aggressive driving task. Videos of the real-world experiments can be found at https://youtu.be/vZST3v0{\_}S9w and https://youtu.be/MhuqiHo2t98.},
archivePrefix = {arXiv},
arxivId = {1902.08967},
author = {Wagener, Nolan and Cheng, Ching An and Sacks, Jacob and Boots, Byron},
doi = {10.15607/rss.2019.xv.033},
eprint = {1902.08967},
file = {::},
journal = {arXiv},
title = {{An online learning approach to model predictive control}},
year = {2019}
}

@article{baril2020evaluation,
      title={Evaluation of Skid-Steering Kinematic Models for Subarctic Environments},
      author={Dominic Baril and Vincent Grondin and Simon-Pierre Deschênes and Johann Laconte and Maxime Vaidis and Vladimír Kubelka and André Gallant and Philippe Giguère and François Pomerleau},
      year={2020},
      eprint={2004.05131},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{Kappen2012,
abstract = {We reformulate a class of non-linear stochastic optimal control problems introduced by Todorov (in Advances in Neural Information Processing Systems, vol. 19, pp. 1369-1376, 2007) as a Kullback-Leibler (KL) minimization problem. As a result, the optimal control computation reduces to an inference computation and approximate inference methods can be applied to efficiently compute approximate optimal controls. We show how this KL control theory contains the path integral control method as a special case. We provide an example of a block stacking task and a multi-agent cooperative game where we demonstrate how approximate inference can be successfully applied to instances that are too complex for exact computation. We discuss the relation of the KL control approach to other inference approaches to control. {\textcopyright} The Author(s) 2012. This article is published with open access at Springerlink.com.},
archivePrefix = {arXiv},
arxivId = {0901.0633},
author = {Kappen, Hilbert J. and G{\'{o}}mez, Vicen{\c{c}} and Opper, Manfred},
doi = {10.1007/s10994-012-5278-7},
eprint = {0901.0633},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Kappen, G{\'{o}}mez, Opper - 2012 - Optimal control as a graphical model inference problem.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Approximate inference,Belief propagation,Cluster variation method,Graphical model,Kullback-leibler divergence,Optimal control,Uncontrolled dynamics},
number = {2},
pages = {159--182},
title = {{Optimal control as a graphical model inference problem}},
volume = {87},
year = {2012}
}

@article{millidge2020relationship,
      title={{On the Relationship Between Active Inference and Control as Inference}},
      author={Beren Millidge and Alexander Tschantz and Anil K Seth and Christopher L Buckley},
      year={2020},
      eprint={2006.12964},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}